{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe75d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prototype \n",
    "\n",
    "import cv2\n",
    "\n",
    "vid = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "        try:\n",
    "            res,frame = vid.read()\n",
    "            cv2.imshow(\"Video\",frame)\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        if cv2.waitKey(1) & 0xff==ord('l'):\n",
    "            break\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5537b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prototype \n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize Mediapipe FaceMesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip frame for natural viewing\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process with FaceMesh\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            # Draw face mesh for visualization\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, face_landmarks, mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                mp_drawing.DrawingSpec(color=(0,255,0), thickness=1, circle_radius=1),\n",
    "                mp_drawing.DrawingSpec(color=(0,0,255), thickness=1, circle_radius=1)\n",
    "            )\n",
    "\n",
    "            # Eye landmarks (Iris tracking)\n",
    "            left_iris = face_landmarks.landmark[468]   # left eye iris center\n",
    "            right_iris = face_landmarks.landmark[473]  # right eye iris center\n",
    "            nose_tip = face_landmarks.landmark[1]      # nose tip (for reference)\n",
    "\n",
    "            h, w, _ = frame.shape\n",
    "            left_x, left_y = int(left_iris.x * w), int(left_iris.y * h)\n",
    "            right_x, right_y = int(right_iris.x * w), int(right_iris.y * h)\n",
    "            nose_x, nose_y = int(nose_tip.x * w), int(nose_tip.y * h)\n",
    "\n",
    "            # Draw points\n",
    "            cv2.circle(frame, (left_x, left_y), 3, (255, 0, 0), -1)\n",
    "            cv2.circle(frame, (right_x, right_y), 3, (255, 0, 0), -1)\n",
    "            cv2.circle(frame, (nose_x, nose_y), 3, (0, 255, 255), -1)\n",
    "\n",
    "            # Simple heuristic: check if iris is aligned with nose (looking forward)\n",
    "            eye_center_x = (left_x + right_x) // 2\n",
    "            if abs(eye_center_x - nose_x) < 20:  \n",
    "                cv2.putText(frame, \"Eye Contact: YES\", (30, 50),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            else:\n",
    "                cv2.putText(frame, \"Eye Contact: NO\", (30, 50),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Eye Contact Tracker\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20c42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prototype \n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True, max_num_faces=1)\n",
    "\n",
    "# Eye landmarks (from Mediapipe FaceMesh)\n",
    "LEFT_EYE = [33, 160, 158, 133, 153, 144]   # left eye region\n",
    "RIGHT_EYE = [362, 385, 387, 263, 373, 380] # right eye region\n",
    "\n",
    "def eye_aspect_ratio(landmarks, eye_points, w, h):\n",
    "    # Get landmark coordinates\n",
    "    p1 = np.array([landmarks[eye_points[1]].x * w, landmarks[eye_points[1]].y * h])\n",
    "    p2 = np.array([landmarks[eye_points[2]].x * w, landmarks[eye_points[2]].y * h])\n",
    "    p3 = np.array([landmarks[eye_points[5]].x * w, landmarks[eye_points[5]].y * h])\n",
    "    p4 = np.array([landmarks[eye_points[4]].x * w, landmarks[eye_points[4]].y * h])\n",
    "    p0 = np.array([landmarks[eye_points[0]].x * w, landmarks[eye_points[0]].y * h])\n",
    "    p3b = np.array([landmarks[eye_points[3]].x * w, landmarks[eye_points[3]].y * h])\n",
    "\n",
    "    # Compute EAR\n",
    "    vertical1 = np.linalg.norm(p2 - p4)\n",
    "    vertical2 = np.linalg.norm(p1 - p3)\n",
    "    horizontal = np.linalg.norm(p0 - p3b)\n",
    "\n",
    "    ear = (vertical1 + vertical2) / (2.0 * horizontal)\n",
    "    return ear\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        h, w, _ = frame.shape\n",
    "        landmarks = results.multi_face_landmarks[0].landmark\n",
    "\n",
    "        # EAR for both eyes\n",
    "        left_ear = eye_aspect_ratio(landmarks, LEFT_EYE, w, h)\n",
    "        right_ear = eye_aspect_ratio(landmarks, RIGHT_EYE, w, h)\n",
    "        avg_ear = (left_ear + right_ear) / 2.0\n",
    "\n",
    "        # Threshold for eye open/close (tune between 0.2â€“0.3)\n",
    "        if avg_ear < 0.25:\n",
    "            status = \"Eyes Closed\"\n",
    "            color = (0, 0, 255)\n",
    "        else:\n",
    "            status = \"Eyes Open\"\n",
    "            color = (0, 255, 0)\n",
    "\n",
    "        cv2.putText(frame, f\"{status}\", (30, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 2)\n",
    "\n",
    "    cv2.imshow(\"Eye State Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4cca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prototype \n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame,1)\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = pose.process(frame_rgb)\n",
    "\n",
    "    if result.pose_landmarks:\n",
    "        landmarks = result.pose_landmarks.landmark\n",
    "\n",
    "        # Get left and right shoulder\n",
    "        l_shoulder = np.array([\n",
    "            landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER].x,\n",
    "            landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER].y\n",
    "        ])\n",
    "        r_shoulder = np.array([\n",
    "            landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER].x,\n",
    "            landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER].y\n",
    "        ])\n",
    "\n",
    "        # Draw circles for shoulders\n",
    "        h, w, _ = frame.shape\n",
    "        cv2.circle(frame, (int(l_shoulder[0] * w), int(l_shoulder[1] * h)), 8, (0, 255, 0), -1)\n",
    "        cv2.circle(frame, (int(r_shoulder[0] * w), int(r_shoulder[1] * h)), 8, (0, 255, 0), -1)\n",
    "\n",
    "        # Calculate angle of shoulder line\n",
    "        dx = r_shoulder[0] - l_shoulder[0]\n",
    "        dy = r_shoulder[1] - l_shoulder[1]\n",
    "        angle = np.degrees(np.arctan2(dy, dx))\n",
    "\n",
    "        # Feedback\n",
    "        if abs(angle) > 10:  # tolerance, straight ~0 degrees\n",
    "            cv2.putText(frame, \"Sit Straight!\", (50, 100),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        else:\n",
    "            cv2.putText(frame, \"Good Posture!\", (50, 100),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Shoulder Posture Feedback\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35dafa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prototype \n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Pose model\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5,\n",
    "                    min_tracking_confidence=0.5)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame,1)\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = pose.process(frame_rgb)\n",
    "\n",
    "    if result.pose_landmarks:\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        # Draw full pose\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame, result.pose_landmarks, mp_pose.POSE_CONNECTIONS\n",
    "        )\n",
    "\n",
    "        # Extract shoulders\n",
    "        left_shoulder = result.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        right_shoulder = result.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "\n",
    "        l_y = left_shoulder.y * h\n",
    "        r_y = right_shoulder.y * h\n",
    "\n",
    "        # Check alignment\n",
    "        if abs(l_y - r_y) > 20:  # pixel difference threshold\n",
    "            cv2.putText(frame, \"Sit Straight!\", (50, 100),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        else:\n",
    "            cv2.putText(frame, \"Good Posture!\", (50, 100),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Posture Feedback\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdca2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prototype \n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize mediapipe modules\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True, max_num_faces=1)\n",
    "\n",
    "# Function to check if eyes are open/closed using eye aspect ratio\n",
    "def eye_aspect_ratio(landmarks, eye_idx, h, w):\n",
    "    left = np.array([landmarks[eye_idx[0]].x * w, landmarks[eye_idx[0]].y * h])\n",
    "    right = np.array([landmarks[eye_idx[3]].x * w, landmarks[eye_idx[3]].y * h])\n",
    "    top = np.array([landmarks[eye_idx[1]].x * w, landmarks[eye_idx[1]].y * h])\n",
    "    bottom = np.array([landmarks[eye_idx[2]].x * w, landmarks[eye_idx[2]].y * h])\n",
    "\n",
    "    horiz = np.linalg.norm(left - right)\n",
    "    vert = np.linalg.norm(top - bottom)\n",
    "    return vert / horiz  # lower ratio = closed\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame,1)\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Pose detection (shoulders)\n",
    "    pose_result = pose.process(frame_rgb)\n",
    "\n",
    "    if pose_result.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, pose_result.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        left_shoulder = pose_result.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        right_shoulder = pose_result.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "\n",
    "        l_y = left_shoulder.y * h\n",
    "        r_y = right_shoulder.y * h\n",
    "\n",
    "        if abs(l_y - r_y) > 20:  # difference in pixel height\n",
    "            cv2.putText(frame, \"Sit Straight!\", (50, 100),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        else:\n",
    "            cv2.putText(frame, \"Good Posture!\", (50, 100),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Face mesh (eyes)\n",
    "    face_result = face_mesh.process(frame_rgb)\n",
    "\n",
    "    if face_result.multi_face_landmarks:\n",
    "        for face_landmarks in face_result.multi_face_landmarks:\n",
    "            landmarks = face_landmarks.landmark\n",
    "\n",
    "            # Indices for left and right eyes (Mediapipe standard)\n",
    "            left_eye_idx = [33, 159, 145, 133]   # left, top, bottom, right\n",
    "            right_eye_idx = [362, 386, 374, 263]\n",
    "\n",
    "            left_ratio = eye_aspect_ratio(landmarks, left_eye_idx, h, w)\n",
    "            right_ratio = eye_aspect_ratio(landmarks, right_eye_idx, h, w)\n",
    "\n",
    "            avg_ratio = (left_ratio + right_ratio) / 2\n",
    "\n",
    "            if avg_ratio < 0.2:  # tweak threshold\n",
    "                cv2.putText(frame, \"Eyes Closed - Stay Focused!\", (50, 150),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            else:\n",
    "                cv2.putText(frame, \"Eyes Open - Good!\", (50, 150),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Posture & Eye Feedback\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4b3439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prototype \n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True, max_num_faces=1)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Iris landmark indices from Mediapipe\n",
    "LEFT_IRIS = [474, 475, 476, 477]\n",
    "RIGHT_IRIS = [469, 470, 471, 472]\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame,1)\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = face_mesh.process(frame_rgb)\n",
    "\n",
    "    if result.multi_face_landmarks:\n",
    "        for face_landmarks in result.multi_face_landmarks:\n",
    "            landmarks = face_landmarks.landmark\n",
    "\n",
    "            # Get iris center for left & right eye\n",
    "            left_iris = [(int(landmarks[i].x * w), int(landmarks[i].y * h)) for i in LEFT_IRIS]\n",
    "            right_iris = [(int(landmarks[i].x * w), int(landmarks[i].y * h)) for i in RIGHT_IRIS]\n",
    "\n",
    "            # Compute center of iris\n",
    "            l_cx = sum([p[0] for p in left_iris]) // 4\n",
    "            l_cy = sum([p[1] for p in left_iris]) // 4\n",
    "            r_cx = sum([p[0] for p in right_iris]) // 4\n",
    "            r_cy = sum([p[1] for p in right_iris]) // 4\n",
    "\n",
    "            # Draw iris centers\n",
    "            cv2.circle(frame, (l_cx, l_cy), 3, (0, 255, 0), -1)\n",
    "            cv2.circle(frame, (r_cx, r_cy), 3, (0, 255, 0), -1)\n",
    "\n",
    "            # Simple check: if iris roughly centered horizontally\n",
    "            if w * 0.35 < l_cx < w * 0.65 and w * 0.35 < r_cx < w * 0.65:\n",
    "                cv2.putText(frame, \"Good Eye Contact!\", (50, 100),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            else:\n",
    "                cv2.putText(frame, \"Not Looking at Camera!\", (50, 100),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Eye Contact Feedback\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb012c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prototype \n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True, max_num_faces=1)\n",
    "\n",
    "# Iris landmark indices\n",
    "LEFT_IRIS = [474, 475, 476, 477]\n",
    "RIGHT_IRIS = [469, 470, 471, 472]\n",
    "\n",
    "# Left & right eye boundary landmarks (Mediapipe reference indices)\n",
    "LEFT_EYE = [33, 133, 159, 145]    # [left, right, top, bottom]\n",
    "RIGHT_EYE = [362, 263, 386, 374]  # [left, right, top, bottom]\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def get_center(points):\n",
    "    x = int(np.mean([p[0] for p in points]))\n",
    "    y = int(np.mean([p[1] for p in points]))\n",
    "    return (x, y)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame,1)\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = face_mesh.process(frame_rgb)\n",
    "\n",
    "    if result.multi_face_landmarks:\n",
    "        landmarks = result.multi_face_landmarks[0].landmark\n",
    "\n",
    "        # Get iris centers\n",
    "        left_iris = [(int(landmarks[i].x * w), int(landmarks[i].y * h)) for i in LEFT_IRIS]\n",
    "        right_iris = [(int(landmarks[i].x * w), int(landmarks[i].y * h)) for i in RIGHT_IRIS]\n",
    "        l_center = get_center(left_iris)\n",
    "        r_center = get_center(right_iris)\n",
    "\n",
    "        # Get eye boundaries\n",
    "        l_eye = [(int(landmarks[i].x * w), int(landmarks[i].y * h)) for i in LEFT_EYE]\n",
    "        r_eye = [(int(landmarks[i].x * w), int(landmarks[i].y * h)) for i in RIGHT_EYE]\n",
    "\n",
    "        # Draw eyes & iris\n",
    "        cv2.circle(frame, l_center, 3, (0, 255, 0), -1)\n",
    "        cv2.circle(frame, r_center, 3, (0, 255, 0), -1)\n",
    "\n",
    "        # Horizontal + vertical check for left eye\n",
    "        lx_min, lx_max = l_eye[0][0], l_eye[1][0]   # left & right\n",
    "        ly_min, ly_max = l_eye[2][1], l_eye[3][1]   # top & bottom\n",
    "\n",
    "        # Horizontal + vertical check for right eye\n",
    "        rx_min, rx_max = r_eye[0][0], r_eye[1][0]\n",
    "        ry_min, ry_max = r_eye[2][1], r_eye[3][1]\n",
    "\n",
    "        # Check if iris is inside both eyes' bounding box\n",
    "        left_inside = (lx_min < l_center[0] < lx_max) and (ly_min < l_center[1] < ly_max)\n",
    "        right_inside = (rx_min < r_center[0] < rx_max) and (ry_min < r_center[1] < ry_max)\n",
    "\n",
    "        if left_inside and right_inside:\n",
    "            cv2.putText(frame, \"Good Eye Contact!\", (50, 100),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        else:\n",
    "            cv2.putText(frame, \"Not Looking at Camera!\", (50, 100),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Eye Contact Feedback\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd72651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prototype \n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame,1)\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = pose.process(frame_rgb)\n",
    "\n",
    "    if result.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, result.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        # Extract key points: nose + shoulders + ears\n",
    "        landmarks = result.pose_landmarks.landmark\n",
    "        nose = landmarks[mp_pose.PoseLandmark.NOSE]\n",
    "        l_ear = landmarks[mp_pose.PoseLandmark.LEFT_EAR]\n",
    "        r_ear = landmarks[mp_pose.PoseLandmark.RIGHT_EAR]\n",
    "\n",
    "        # Convert to pixel coords\n",
    "        nose = (int(nose.x * w), int(nose.y * h))\n",
    "        l_ear = (int(l_ear.x * w), int(l_ear.y * h))\n",
    "        r_ear = (int(r_ear.x * w), int(r_ear.y * h))\n",
    "\n",
    "        # Draw points\n",
    "        cv2.circle(frame, nose, 5, (0, 255, 0), -1)\n",
    "        cv2.circle(frame, l_ear, 5, (255, 0, 0), -1)\n",
    "        cv2.circle(frame, r_ear, 5, (255, 0, 0), -1)\n",
    "\n",
    "        # Head tilt check â†’ difference in ear height\n",
    "        ear_diff = abs(l_ear[1] - r_ear[1])\n",
    "\n",
    "        # Forward/backward tilt â†’ distance between nose and ear line\n",
    "        ear_mid_x = (l_ear[0] + r_ear[0]) // 2\n",
    "        forward_tilt = abs(nose[0] - ear_mid_x)\n",
    "\n",
    "        # Thresholds\n",
    "        if ear_diff < 30 and forward_tilt < 50:  # tweak as needed\n",
    "            cv2.putText(frame, \"Good Head Position!\", (50, 100),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        else:\n",
    "            cv2.putText(frame, \"Adjust Head Position!\", (50, 100),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Head Position Feedback\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2c36bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prototype \n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Mediapipe face mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False,\n",
    "                                  max_num_faces=1,\n",
    "                                  refine_landmarks=True,\n",
    "                                  min_detection_confidence=0.5,\n",
    "                                  min_tracking_confidence=0.5)\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for landmarks in results.multi_face_landmarks:\n",
    "            h, w, _ = frame.shape\n",
    "\n",
    "            # Four key points\n",
    "            forehead = landmarks.landmark[10]      # near top of forehead\n",
    "            chin = landmarks.landmark[152]        # bottom of chin\n",
    "            left_cheek = landmarks.landmark[234]  # left cheekbone\n",
    "            right_cheek = landmarks.landmark[454] # right cheekbone\n",
    "\n",
    "            # Convert to pixel coords\n",
    "            f = (int(forehead.x * w), int(forehead.y * h))\n",
    "            c = (int(chin.x * w), int(chin.y * h))\n",
    "            l = (int(left_cheek.x * w), int(left_cheek.y * h))\n",
    "            r = (int(right_cheek.x * w), int(right_cheek.y * h))\n",
    "\n",
    "            # Draw points\n",
    "            for point in [f, c, l, r]:\n",
    "                cv2.circle(frame, point, 5, (0,255,0), -1)\n",
    "\n",
    "            # Check vertical alignment forehead-chin\n",
    "            vertical_alignment = abs(f[0] - c[0])  # X difference\n",
    "            # Check left-right balance\n",
    "            left_right_balance = abs((l[1] - r[1]))  # Y difference\n",
    "\n",
    "            # Tolerances\n",
    "            if vertical_alignment < 30 and left_right_balance < 20:\n",
    "                text = \"Good: Head straight\"\n",
    "                color = (0,255,0)\n",
    "            else:\n",
    "                text = \"Not good: Sit straight\"\n",
    "                color = (0,0,255)\n",
    "\n",
    "            cv2.putText(frame, text, (50,50), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        1, color, 2)\n",
    "\n",
    "    cv2.imshow(\"Head Alignment Check\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3621b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prototype \n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Face + Pose\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False,\n",
    "                                  max_num_faces=1,\n",
    "                                  refine_landmarks=True,\n",
    "                                  min_detection_confidence=0.5,\n",
    "                                  min_tracking_confidence=0.5)\n",
    "\n",
    "pose = mp_pose.Pose(static_image_mode=False,\n",
    "                    min_detection_confidence=0.5,\n",
    "                    min_tracking_confidence=0.5)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process with face + pose\n",
    "    results_face = face_mesh.process(rgb)\n",
    "    results_pose = pose.process(rgb)\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "\n",
    "    if results_face.multi_face_landmarks and results_pose.pose_landmarks:\n",
    "        face_landmarks = results_face.multi_face_landmarks[0]\n",
    "        pose_landmarks = results_pose.pose_landmarks.landmark\n",
    "\n",
    "        # ---- FACE POINTS ----\n",
    "        forehead = face_landmarks.landmark[10]\n",
    "        chin = face_landmarks.landmark[152]\n",
    "        left_cheek = face_landmarks.landmark[234]\n",
    "        right_cheek = face_landmarks.landmark[454]\n",
    "\n",
    "        f = (int(forehead.x * w), int(forehead.y * h))\n",
    "        c = (int(chin.x * w), int(chin.y * h))\n",
    "        l = (int(left_cheek.x * w), int(left_cheek.y * h))\n",
    "        r = (int(right_cheek.x * w), int(right_cheek.y * h))\n",
    "\n",
    "        # ---- CHEST MIDPOINT ----\n",
    "        left_shoulder = pose_landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        right_shoulder = pose_landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "\n",
    "        chest_x = int((left_shoulder.x + right_shoulder.x) / 2 * w)\n",
    "        chest_y = int((left_shoulder.y + right_shoulder.y) / 2 * h)\n",
    "        chest = (chest_x, chest_y)\n",
    "\n",
    "        # Draw landmarks\n",
    "        for point in [f, c, l, r, chest]:\n",
    "            cv2.circle(frame, point, 5, (0, 255, 0), -1)\n",
    "\n",
    "        # ---- CHECK ALIGNMENTS ----\n",
    "        vertical_alignment = abs(f[0] - c[0])   # forehead-chin x-diff\n",
    "        cheek_balance = abs(l[1] - r[1])        # cheek height difference\n",
    "        chin_chest_x = abs(c[0] - chest[0])     # chin vs chest alignment (X)\n",
    "        chin_chest_y = abs(c[1] - chest[1])     # chin vs chest vertical gap\n",
    "\n",
    "        # Tolerances\n",
    "        if vertical_alignment < 25 and cheek_balance < 20 and chin_chest_x < 30 and 50 < chin_chest_y < 150:\n",
    "            text = \"Good: Head & Posture straight\"\n",
    "            color = (0, 255, 0)\n",
    "        else:\n",
    "            text = \"Not good: Sit straight\"\n",
    "            color = (0, 0, 255)\n",
    "\n",
    "        cv2.putText(frame, text, (50, 50), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1, color, 2)\n",
    "\n",
    "    cv2.imshow(\"Head + Posture Alignment\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efe3a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prototype \n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "\n",
    "# Face + Pose\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False,\n",
    "                                  max_num_faces=1,\n",
    "                                  refine_landmarks=True,\n",
    "                                  min_detection_confidence=0.5,\n",
    "                                  min_tracking_confidence=0.5)\n",
    "\n",
    "pose = mp_pose.Pose(static_image_mode=False,\n",
    "                    min_detection_confidence=0.5,\n",
    "                    min_tracking_confidence=0.5)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "baseline_chin_chest_dist = None  # Neutral reference distance\n",
    "\n",
    "def euclidean(p1, p2):\n",
    "    return math.sqrt((p1[0]-p2[0])**2 + (p1[1]-p2[1])**2)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process face + pose\n",
    "    results_face = face_mesh.process(rgb)\n",
    "    results_pose = pose.process(rgb)\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "\n",
    "    if results_face.multi_face_landmarks and results_pose.pose_landmarks:\n",
    "        face_landmarks = results_face.multi_face_landmarks[0]\n",
    "        pose_landmarks = results_pose.pose_landmarks.landmark\n",
    "\n",
    "        # Face: chin\n",
    "        chin = face_landmarks.landmark[152]\n",
    "        c = (int(chin.x * w), int(chin.y * h))\n",
    "\n",
    "        # Pose: chest midpoint from shoulders\n",
    "        left_shoulder = pose_landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        right_shoulder = pose_landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "        chest_x = int((left_shoulder.x + right_shoulder.x) / 2 * w)\n",
    "        chest_y = int((left_shoulder.y + right_shoulder.y) / 2 * h)\n",
    "        chest = (chest_x, chest_y)\n",
    "\n",
    "        # Draw points\n",
    "        cv2.circle(frame, c, 5, (255, 0, 0), -1)\n",
    "        cv2.circle(frame, chest, 5, (0, 255, 0), -1)\n",
    "        cv2.line(frame, c, chest, (255, 255, 0), 2)\n",
    "\n",
    "        # Distance\n",
    "        chin_chest_dist = euclidean(c, chest)\n",
    "\n",
    "        if baseline_chin_chest_dist is None:\n",
    "            baseline_chin_chest_dist = chin_chest_dist  # set at start\n",
    "\n",
    "        # Compare with baseline\n",
    "        ratio = chin_chest_dist / baseline_chin_chest_dist\n",
    "\n",
    "        if ratio > 1.25:  # 25% more â†’ looking up\n",
    "            feedback = \"Not good: Looking up\"\n",
    "            color = (0, 0, 255)\n",
    "        elif ratio < 0.75:  # 25% less â†’ looking down\n",
    "            feedback = \"Not good: Looking down\"\n",
    "            color = (0, 0, 255)\n",
    "        else:\n",
    "            feedback = \"Good: Head aligned\"\n",
    "            color = (0, 255, 0)\n",
    "\n",
    "        cv2.putText(frame, feedback, (50, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "    cv2.imshow(\"Head + Chest Alignment\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c33f78da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------- THE FINAL VERDICT ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "\n",
    "# Face + Pose\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False,\n",
    "                                  max_num_faces=1,\n",
    "                                  refine_landmarks=True,\n",
    "                                  min_detection_confidence=0.5,\n",
    "                                  min_tracking_confidence=0.5)\n",
    "\n",
    "pose = mp_pose.Pose(static_image_mode=False,\n",
    "                    min_detection_confidence=0.5,\n",
    "                    min_tracking_confidence=0.5)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "baseline_chin_chest_dist = None  # Neutral reference distance\n",
    "\n",
    "def euclidean(p1, p2):\n",
    "    return math.sqrt((p1[0]-p2[0])**2 + (p1[1]-p2[1])**2)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process face + pose\n",
    "    results_face = face_mesh.process(rgb)\n",
    "    results_pose = pose.process(rgb)\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "\n",
    "    if results_face.multi_face_landmarks and results_pose.pose_landmarks:\n",
    "        face_landmarks = results_face.multi_face_landmarks[0]\n",
    "        pose_landmarks = results_pose.pose_landmarks.landmark\n",
    "\n",
    "        # ---- FACE POINTS ----\n",
    "        forehead = face_landmarks.landmark[10]\n",
    "        chin = face_landmarks.landmark[152]\n",
    "        left_cheek = face_landmarks.landmark[234]\n",
    "        right_cheek = face_landmarks.landmark[454]\n",
    "\n",
    "        f = (int(forehead.x * w), int(forehead.y * h))\n",
    "        c = (int(chin.x * w), int(chin.y * h))\n",
    "        l = (int(left_cheek.x * w), int(left_cheek.y * h))\n",
    "        r = (int(right_cheek.x * w), int(right_cheek.y * h))\n",
    "\n",
    "        # ---- SHOULDERS + CHEST ----\n",
    "        left_shoulder = pose_landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        right_shoulder = pose_landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "\n",
    "        ls = (int(left_shoulder.x * w), int(left_shoulder.y * h))\n",
    "        rs = (int(right_shoulder.x * w), int(right_shoulder.y * h))\n",
    "        chest = (int((ls[0] + rs[0]) / 2), int((ls[1] + rs[1]) / 2))\n",
    "\n",
    "        # Draw landmarks\n",
    "        for pt in [f, c, l, r, ls, rs, chest]:\n",
    "            cv2.circle(frame, pt, 5, (0, 255, 0), -1)\n",
    "\n",
    "        # ---- HEAD STRAIGHT CHECK ----\n",
    "        vertical_alignment = abs(f[0] - c[0])   # forehead-chin x-diff\n",
    "        cheek_balance = abs(l[1] - r[1])        # cheek height difference\n",
    "\n",
    "        head_good = vertical_alignment < 25 and cheek_balance < 20\n",
    "\n",
    "        # ---- SHOULDER ALIGNMENT ----\n",
    "        shoulder_balance = abs(ls[1] - rs[1])\n",
    "        shoulders_good = shoulder_balance < 20\n",
    "\n",
    "        # ---- CHINâ€“CHEST DISTANCE ----\n",
    "        chin_chest_dist = euclidean(c, chest)\n",
    "        if baseline_chin_chest_dist is None:\n",
    "            baseline_chin_chest_dist = chin_chest_dist\n",
    "\n",
    "        ratio = chin_chest_dist / baseline_chin_chest_dist\n",
    "        if ratio > 1.25:\n",
    "            chin_status = \"Looking up\"\n",
    "            chin_good = False\n",
    "        elif ratio < 0.75:\n",
    "            chin_status = \"Looking down\"\n",
    "            chin_good = False\n",
    "        else:\n",
    "            chin_status = \"Head aligned\"\n",
    "            chin_good = True\n",
    "\n",
    "        # ---- FINAL FEEDBACK ----\n",
    "        feedback_lines = []\n",
    "        if head_good:\n",
    "            feedback_lines.append(\"GOOD HEAD POSITION\")\n",
    "        else:\n",
    "            feedback_lines.append(\"NOT GOOD HEAD POSITION \")\n",
    "\n",
    "        if shoulders_good:\n",
    "            feedback_lines.append(\"GOOD SHOULDER ALLIGNMENT\")\n",
    "        else:\n",
    "            feedback_lines.append(\"NOT GOOD SHOULDER ALIGNMENT\")\n",
    "\n",
    "        if chin_good:\n",
    "            feedback_lines.append(\"GOOD \" + chin_status)\n",
    "        else:\n",
    "            feedback_lines.append(\"NOT GOOD \" + chin_status)\n",
    "\n",
    "        # Display feedback\n",
    "        y0 = 20\n",
    "        for line in feedback_lines:\n",
    "            cv2.putText(frame, line, (10, y0),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255) if \"NOT GOOD\" in line else (0, 255, 0), 2)\n",
    "            y0 += 35\n",
    "\n",
    "    cv2.imshow(\"Posture & Head Feedback\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39051137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\mock-AI\\myvenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sentence_transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27adbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== INTERVIEW FEEDBACK REPORT =====\n",
      "Question: What is machine learning?\n",
      "Answer: no i dont know\n",
      "\n",
      "ðŸ”¹ Semantic Similarity: 0.04 (0â€“1 scale)\n",
      "ðŸ”¹ Clarity Score: 0.50\n",
      "\n",
      "âœ… Final Interview Answer Score: 42.14/100\n",
      "âš ï¸ Your answer was off-topic. Try addressing the question more directly.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import re\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Sample Q&A\n",
    "question = \"What is machine learning?\"\n",
    "answer = \"\"\"no i dont know\"\"\"\n",
    "\n",
    "# 1. Semantic Similarity\n",
    "q_emb = model.encode(question, convert_to_tensor=True)\n",
    "a_emb = model.encode(answer, convert_to_tensor=True)\n",
    "\n",
    "similarity = util.pytorch_cos_sim(q_emb, a_emb).item()\n",
    "# 3. Clarity/Length Check (very basic)\n",
    "num_words = len(answer.split())\n",
    "clarity_score = 1.0 if 30 <= num_words <= 120 else 0.5 if num_words < 30 else 0.7\n",
    "\n",
    "# 4. Final Composite Score\n",
    "final_score = round((0.5 * similarity + 0.3 + 0.2 * clarity_score) * 100, 2)\n",
    "\n",
    "# 5. Generate Feedback\n",
    "print(\"\\n===== INTERVIEW FEEDBACK REPORT =====\")\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer.strip()}\")\n",
    "print(f\"\\nðŸ”¹ Semantic Similarity: {similarity:.2f} (0â€“1 scale)\")\n",
    "\n",
    "print(f\"ðŸ”¹ Clarity Score: {clarity_score:.2f}\")\n",
    "print(f\"\\nâœ… Final Interview Answer Score: {final_score}/100\")\n",
    "\n",
    "if similarity < 0.5:\n",
    "    print(\"âš ï¸ Your answer was off-topic. Try addressing the question more directly.\")\n",
    "\n",
    "\n",
    "elif clarity_score < 0.7:\n",
    "    print(\"âš ï¸ Your answer length/clarity could be improved.\")\n",
    "else:\n",
    "    print(\"ðŸŽ‰ Great job! Your answer was relevant, complete, and clear.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abee107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5c72919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load Hugging Face sentiment pipeline\n",
    "sentiment_model = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "def analyze_tone(audio_path: str, transcript: str):\n",
    "    # --- AUDIO FEATURE ANALYSIS ---\n",
    "    y, sr = librosa.load(audio_path)\n",
    "\n",
    "    # Pitch (fundamental frequency)\n",
    "    pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "    pitch_values = pitches[pitches > 0]\n",
    "    avg_pitch = np.mean(pitch_values) if len(pitch_values) > 0 else 0\n",
    "\n",
    "    # Volume (RMS energy)\n",
    "    rms = np.mean(librosa.feature.rms(y=y))\n",
    "\n",
    "    # Speech rate proxy (tempo in beats per minute)\n",
    "    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "\n",
    "    # Normalize values (simple scaling to 0â€“1 range)\n",
    "    pitch_score = min(avg_pitch / 300, 1.0)         # human voice usually < 300 Hz\n",
    "    volume_score = min(rms * 50, 1.0)               # scaling RMS\n",
    "    rate_score = min(tempo / 200, 1.0)              # normal ~100â€“160 bpm\n",
    "\n",
    "    audio_confidence = (0.4 * pitch_score +\n",
    "                        0.4 * volume_score +\n",
    "                        0.2 * rate_score)\n",
    "\n",
    "    # --- TEXT SENTIMENT ANALYSIS ---\n",
    "    sentiment = sentiment_model(transcript)[0]\n",
    "    text_score = sentiment[\"score\"] if sentiment[\"label\"] == \"POSITIVE\" else (1 - sentiment[\"score\"])\n",
    "\n",
    "    # --- FINAL TONE SCORE ---\n",
    "    tone_score = int(100 * (0.6 * audio_confidence + 0.4 * text_score))\n",
    "\n",
    "    # --- INTERPRETATION ---\n",
    "    if tone_score >= 75:\n",
    "        label = \"Confident\"\n",
    "        feedback = \"Great energy and clarity. Maintain this tone!\"\n",
    "    elif tone_score >= 50:\n",
    "        label = \"Neutral\"\n",
    "        feedback = \"Tone was okay, but try to add more variation and confidence in your delivery.\"\n",
    "    else:\n",
    "        label = \"Uncertain\"\n",
    "        feedback = \"Your tone seemed hesitant. Speak louder, vary your pitch, and use confident wording.\"\n",
    "\n",
    "    return {\n",
    "        \"tone_score\": tone_score,\n",
    "        \"label\": label,\n",
    "        \"audio_metrics\": {\n",
    "            \"avg_pitch_hz\": round(float(avg_pitch), 2),\n",
    "            \"volume_rms\": round(float(rms), 4),\n",
    "            \"speech_rate_bpm\": tempo\n",
    "        },\n",
    "        \"sentiment\": sentiment,\n",
    "        \"feedback\": feedback\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6991e3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tone_score': 56, 'label': 'Neutral', 'audio_metrics': {'avg_pitch_hz': 1715.78, 'volume_rms': 0.0794, 'speech_rate_bpm': array([135.99917763])}, 'sentiment': {'label': 'NEGATIVE', 'score': 0.9895516037940979}, 'feedback': 'Tone was okay, but try to add more variation and confidence in your delivery.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suees\\AppData\\Local\\Temp\\ipykernel_20884\\3975715245.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  tone_score = int(100 * (0.6 * audio_confidence + 0.4 * text_score))\n"
     ]
    }
   ],
   "source": [
    "result = analyze_tone(\"hello_example.wav\", \n",
    "   \"Hello, this is an example of speech being saved into a WAV file.\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c07f290e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tone_score': 56,\n",
       " 'label': 'Neutral',\n",
       " 'audio_metrics': {'avg_pitch_hz': 1715.78,\n",
       "  'volume_rms': 0.0794,\n",
       "  'speech_rate_bpm': array([135.99917763])},\n",
       " 'sentiment': {'label': 'NEGATIVE', 'score': 0.9895516037940979},\n",
       " 'feedback': 'Tone was okay, but try to add more variation and confidence in your delivery.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0336240a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d501ff1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
